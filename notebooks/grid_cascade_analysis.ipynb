{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "title_and_objectives"
   },
   "source": [
    "# GridGuard: Cascade Failure Analysis with Graph Neural Networks\n",
    "\n",
    "This notebook trains a Graph Convolutional Network (GCN) to identify cascade failure patterns in power grid topology and runs batch inference across all simulation scenarios.\n",
    "\n",
    "**Objectives:**\n",
    "1. Load grid topology and telemetry data from Snowflake\n",
    "2. Build PyTorch Geometric graph structures\n",
    "3. Train a GCN model on historical cascade patterns\n",
    "4. Run batch inference for all scenarios (BASE_CASE, HIGH_LOAD, WINTER_STORM_2021)\n",
    "5. Identify \"Patient Zero\" - the cascade origin node\n",
    "6. Write results to SIMULATION_RESULTS table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "install_packages"
   },
   "outputs": [],
   "source": [
    "# install_packages: Install PyTorch Geometric and dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Use os.system for pip installs (subprocess doesn't work in SPCS headless mode)\n",
    "# torch: Deep learning framework (provides tensors, autograd, neural network modules)\n",
    "# torch-geometric: PyTorch extension for graph neural networks\n",
    "# networkx: Graph analysis library (for BFS, shortest paths, etc.)\n",
    "# scikit-learn: For model evaluation metrics (confusion matrix, precision, recall)\n",
    "# matplotlib: For visualization of training curves and results\n",
    "os.system(f\"{sys.executable} -m pip install torch torch-geometric networkx scikit-learn matplotlib -q\")\n",
    "\n",
    "print(\"Package installation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# import_libraries: Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and PyG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# Snowflake session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "snowflake_session_setup"
   },
   "outputs": [],
   "source": [
    "# snowflake_session_setup: Get and verify Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "# Verify connection\n",
    "result = session.sql(\"SELECT CURRENT_DATABASE(), CURRENT_SCHEMA(), CURRENT_USER()\").collect()\n",
    "print(f\"Database: {result[0][0]}\")\n",
    "print(f\"Schema: {result[0][1]}\")\n",
    "print(f\"User: {result[0][2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_grid_topology"
   },
   "outputs": [],
   "source": [
    "# load_grid_topology: Load grid nodes and edges from Snowflake\n",
    "\n",
    "# Load grid nodes\n",
    "nodes_df = session.sql(\"\"\"\n",
    "    SELECT NODE_ID, NODE_NAME, NODE_TYPE, LAT, LON, REGION, \n",
    "           CAPACITY_MW, VOLTAGE_KV, CRITICALITY_SCORE\n",
    "    FROM GRID_NODES\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(nodes_df)} grid nodes\")\n",
    "print(f\"Node types: {nodes_df['NODE_TYPE'].value_counts().to_dict()}\")\n",
    "print(f\"Regions: {nodes_df['REGION'].unique().tolist()}\")\n",
    "\n",
    "# Load grid edges\n",
    "edges_df = session.sql(\"\"\"\n",
    "    SELECT EDGE_ID, SRC_NODE, DST_NODE, EDGE_TYPE, \n",
    "           CAPACITY_MW, LENGTH_MILES, VOLTAGE_KV, REDUNDANCY_LEVEL\n",
    "    FROM GRID_EDGES\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(edges_df)} grid edges\")\n",
    "print(f\"Edge types: {edges_df['EDGE_TYPE'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_telemetry_data"
   },
   "outputs": [],
   "source": [
    "# load_telemetry_data: Load historical telemetry for all scenarios\n",
    "telemetry_df = session.sql(\"\"\"\n",
    "    SELECT TELEMETRY_ID, TIMESTAMP, NODE_ID, SCENARIO_NAME,\n",
    "           VOLTAGE_KV, LOAD_MW, FREQUENCY_HZ, TEMPERATURE_F, \n",
    "           STATUS, ALERT_CODE\n",
    "    FROM HISTORICAL_TELEMETRY\n",
    "    ORDER BY SCENARIO_NAME, TIMESTAMP, NODE_ID\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(telemetry_df)} telemetry records\")\n",
    "print(f\"Scenarios: {telemetry_df['SCENARIO_NAME'].unique().tolist()}\")\n",
    "print(f\"Status distribution:\")\n",
    "print(telemetry_df.groupby(['SCENARIO_NAME', 'STATUS']).size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "create_node_mappings"
   },
   "outputs": [],
   "source": [
    "# create_node_mappings: Create node ID to index mapping for PyG\n",
    "node_ids = nodes_df['NODE_ID'].tolist()\n",
    "node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "idx_to_node = {idx: node_id for node_id, idx in node_to_idx.items()}\n",
    "\n",
    "num_nodes = len(node_ids)\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Sample mapping: {list(node_to_idx.items())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph Structure: Edge Index\n",
    "\n",
    "PyTorch Geometric (PyG) represents graphs using a **COO (Coordinate) sparse format** for edges, stored in a tensor called `edge_index`.\n",
    "\n",
    "### Edge Index Format\n",
    "- Shape: `[2, num_edges]` where:\n",
    "  - Row 0 contains **source node indices**\n",
    "  - Row 1 contains **destination node indices**\n",
    "- Example: `[[0, 1, 2], [1, 2, 0]]` means edges: 0→1, 1→2, 2→0\n",
    "\n",
    "### Why Duplicate Edges for Undirected Graphs?\n",
    "Power grids are **undirected** (electricity flows both ways on transmission lines). PyG's GCNConv expects directed edges, so we add both directions:\n",
    "- Original edge: Substation A → Substation B\n",
    "- We add: A→B **and** B→A\n",
    "\n",
    "This ensures message passing flows symmetrically during graph convolution.\n",
    "\n",
    "### The `.t().contiguous()` Pattern\n",
    "```python\n",
    "edge_index = torch.tensor(edge_list).t().contiguous()\n",
    "```\n",
    "- `.t()` — Transpose from `[num_edges, 2]` to `[2, num_edges]` (PyG's expected format)\n",
    "- `.contiguous()` — Ensures memory is laid out sequentially (required for efficient GPU operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "build_edge_index"
   },
   "outputs": [],
   "source": [
    "# build_edge_index: Build edge index for PyTorch Geometric\n",
    "edge_list = []\n",
    "for _, row in edges_df.iterrows():\n",
    "    src_idx = node_to_idx.get(row['SRC_NODE'])\n",
    "    dst_idx = node_to_idx.get(row['DST_NODE'])\n",
    "    if src_idx is not None and dst_idx is not None:\n",
    "        # Add both directions for undirected graph\n",
    "        edge_list.append([src_idx, dst_idx])\n",
    "        edge_list.append([dst_idx, src_idx])\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Number of edges (directed): {edge_index.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "define_feature_function"
   },
   "outputs": [],
   "source": [
    "# define_feature_function: Define function to create node feature matrix\n",
    "def create_node_features(nodes_df, telemetry_snapshot):\n",
    "    \"\"\"\n",
    "    Create node feature matrix from topology and telemetry data.\n",
    "    \n",
    "    Returns a tensor of shape [num_nodes, 10] where each row contains:\n",
    "    \n",
    "    CONTINUOUS FEATURES (indices 0-5):\n",
    "    - [0] Normalized capacity: max generation/load capacity, indicates node importance\n",
    "    - [1] Normalized voltage: operating voltage level, higher = more critical infrastructure\n",
    "    - [2] Criticality score: pre-computed importance metric (0-1)\n",
    "    - [3] Load ratio: current_load / capacity — values > 1.0 indicate overload stress\n",
    "    - [4] Temperature: ambient conditions affect equipment failure rates\n",
    "    - [5] Status encoding: ordinal encoding of operational state\n",
    "    \n",
    "    CATEGORICAL FEATURES (indices 6-9):\n",
    "    - [6-9] Node type one-hot: SUBSTATION, GENERATOR, LOAD_CENTER, TRANSMISSION_HUB\n",
    "    \n",
    "    DESIGN DECISIONS:\n",
    "    - Features are normalized to roughly [0, 1] range to help gradient-based optimization\n",
    "    - Status uses ordinal encoding (ACTIVE=0 < WARNING=0.5 < OFFLINE=0.8 < FAILED=1.0)\n",
    "      instead of one-hot because there's a natural severity ordering\n",
    "    - Load ratio is capped at 2.0 to prevent extreme outliers from dominating\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Status mapping: ordinal encoding preserves severity ordering\n",
    "    # This is preferable to one-hot when categories have natural order\n",
    "    status_map = {'ACTIVE': 0.0, 'WARNING': 0.5, 'FAILED': 1.0, 'OFFLINE': 0.8}\n",
    "    \n",
    "    # Node type one-hot: no natural ordering, so use one-hot encoding\n",
    "    # This prevents the model from learning spurious ordinal relationships\n",
    "    type_map = {'SUBSTATION': [1,0,0,0], 'GENERATOR': [0,1,0,0], \n",
    "                'LOAD_CENTER': [0,0,1,0], 'TRANSMISSION_HUB': [0,0,0,1]}\n",
    "    \n",
    "    for _, node in nodes_df.iterrows():\n",
    "        node_id = node['NODE_ID']\n",
    "        \n",
    "        # Get latest telemetry for this node (temporal snapshot)\n",
    "        node_telemetry = telemetry_snapshot[telemetry_snapshot['NODE_ID'] == node_id]\n",
    "        \n",
    "        if len(node_telemetry) > 0:\n",
    "            latest = node_telemetry.iloc[-1]\n",
    "            load_mw = latest['LOAD_MW'] if pd.notna(latest['LOAD_MW']) else 0\n",
    "            temp_f = latest['TEMPERATURE_F'] if pd.notna(latest['TEMPERATURE_F']) else 70\n",
    "            status = status_map.get(latest['STATUS'], 0.0)\n",
    "        else:\n",
    "            # Default values for nodes without telemetry (use domain-reasonable defaults)\n",
    "            load_mw = 0\n",
    "            temp_f = 70  # Room temperature baseline\n",
    "            status = 0.0  # Assume active if no data\n",
    "        \n",
    "        # Static topology features (from GRID_NODES table)\n",
    "        capacity = node['CAPACITY_MW'] if pd.notna(node['CAPACITY_MW']) else 500\n",
    "        voltage = node['VOLTAGE_KV'] if pd.notna(node['VOLTAGE_KV']) else 138\n",
    "        criticality = node['CRITICALITY_SCORE'] if pd.notna(node['CRITICALITY_SCORE']) else 0.5\n",
    "        \n",
    "        # Assemble feature vector with normalization\n",
    "        base_features = [\n",
    "            capacity / 2000,  # Normalize by typical max capacity (2000 MW)\n",
    "            voltage / 500,    # Normalize by max transmission voltage (500 kV)\n",
    "            criticality,      # Already in [0, 1] range\n",
    "            min(load_mw / max(capacity, 1), 2.0),  # Load ratio; cap at 2x to limit outliers\n",
    "            (temp_f - 32) / 100,  # Convert to ~[0, 1] range (32°F=0, 132°F=1)\n",
    "            status            # Already in [0, 1] from ordinal mapping\n",
    "        ]\n",
    "        \n",
    "        # Append one-hot node type vector\n",
    "        type_vec = type_map.get(node['NODE_TYPE'], [0,0,0,0])\n",
    "        \n",
    "        features.append(base_features + type_vec)\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "print(\"Feature creation function defined\")\n",
    "print(\"Features: capacity, voltage, criticality, load_ratio, temperature, status, type_onehot(4)\")\n",
    "print(\"Total features per node: 10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Networks (GCNs): Core Concepts\n",
    "\n",
    "### Why Use GCNs for Power Grid Analysis?\n",
    "\n",
    "Traditional machine learning treats each node independently, ignoring **network topology**. This is problematic for cascade failure prediction because:\n",
    "- A node's failure risk depends heavily on its **neighbors' states**\n",
    "- Failures propagate through **physical connections** (transmission lines)\n",
    "- Network structure determines which nodes are \"upstream\" or \"downstream\" of stress points\n",
    "\n",
    "GCNs solve this by learning representations that incorporate both **node features** and **graph structure**.\n",
    "\n",
    "### How GCNs Work: Message Passing\n",
    "\n",
    "Each GCN layer performs **neighborhood aggregation**:\n",
    "\n",
    "1. **Gather**: Each node collects feature vectors from its neighbors\n",
    "2. **Aggregate**: Combine neighbor features (typically via mean/sum)\n",
    "3. **Transform**: Apply a learned linear transformation + nonlinearity\n",
    "\n",
    "Mathematically, the GCN layer update rule is:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{A} = A + I$ — Adjacency matrix with self-loops (so nodes also consider their own features)\n",
    "- $\\tilde{D}$ — Degree matrix of $\\tilde{A}$ (diagonal matrix of node degrees)\n",
    "- $H^{(l)}$ — Node features at layer $l$\n",
    "- $W^{(l)}$ — Learnable weight matrix\n",
    "- $\\sigma$ — Nonlinear activation (ReLU)\n",
    "\n",
    "The $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ term is called **symmetric normalization** — it prevents nodes with many neighbors from dominating.\n",
    "\n",
    "### Receptive Field and Layer Depth\n",
    "\n",
    "Each GCN layer lets a node \"see\" one hop further:\n",
    "- **1 layer**: Node sees immediate neighbors\n",
    "- **2 layers**: Node sees 2-hop neighborhood\n",
    "- **3 layers**: Node sees 3-hop neighborhood (used here)\n",
    "\n",
    "**Warning**: Too many layers cause **over-smoothing** — all node representations converge to the same vector. 3 layers is a common sweet spot for medium-sized graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "define_gcn_model"
   },
   "outputs": [],
   "source": [
    "# define_gcn_model: Define the Graph Convolutional Network model\n",
    "class CascadeGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network for cascade failure prediction.\n",
    "    \n",
    "    ARCHITECTURE OVERVIEW:\n",
    "    ┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────┐\n",
    "    │ GCNConv(64) │ --> │ GCNConv(64) │ --> │ GCNConv(32) │ --> │ FC(1)   │\n",
    "    │ + ReLU      │     │ + ReLU      │     │ + ReLU      │     │ +Sigmoid│\n",
    "    │ + Dropout   │     │ + Dropout   │     │             │     │         │\n",
    "    └─────────────┘     └─────────────┘     └─────────────┘     └─────────┘\n",
    "    \n",
    "    Input: [num_nodes, 10] node features\n",
    "    Output: [num_nodes] failure probabilities in (0, 1)\n",
    "    \n",
    "    WHY 3 LAYERS?\n",
    "    - Each GCN layer expands the receptive field by 1 hop\n",
    "    - 3 layers = each node aggregates info from 3-hop neighborhood\n",
    "    - In power grids, cascade effects typically propagate through nearby substations\n",
    "    - More layers risk \"over-smoothing\" where all nodes converge to similar embeddings\n",
    "    \n",
    "    WHY THESE DIMENSIONS?\n",
    "    - 10 → 64: Expand to learn richer representations\n",
    "    - 64 → 64: Maintain capacity for complex patterns\n",
    "    - 64 → 32: Compress before final prediction (bottleneck forces abstraction)\n",
    "    - 32 → 1: Binary classification output\n",
    "    \n",
    "    REGULARIZATION:\n",
    "    - Dropout (30%): Randomly zeros neurons during training to prevent overfitting\n",
    "    - Applied after layers 1 and 2, but NOT after layer 3 (preserve final features)\n",
    "    - Weight decay in optimizer provides L2 regularization on weights\n",
    "    \n",
    "    OUTPUT ACTIVATION:\n",
    "    - Sigmoid squashes output to (0, 1) for probability interpretation\n",
    "    - Enables direct interpretation as P(failure | node features, graph structure)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, hidden_dim=64, dropout=0.3):\n",
    "        super(CascadeGCN, self).__init__()\n",
    "        \n",
    "        # Layer 1: Expand from input features to hidden dimension\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        # Layer 2: Maintain hidden dimension for further message passing\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        # Layer 3: Compress to bottleneck dimension\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Dropout for regularization (prevents co-adaptation of neurons)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer: project from embedding to scalar prediction\n",
    "        self.fc = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1: Transform + aggregate 1-hop neighbors\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)  # Non-linearity enables learning complex patterns\n",
    "        x = self.dropout(x)  # Regularization during training\n",
    "        \n",
    "        # Layer 2: Aggregate 2-hop neighborhood\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 3: Final aggregation (3-hop), no dropout to preserve learned features\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Project to failure probability\n",
    "        # sigmoid(x) maps any real number to (0, 1)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        return out.squeeze(-1)  # Remove last dimension: [N, 1] -> [N]\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CascadeGCN(num_features=10, hidden_dim=64, dropout=0.3).to(device)\n",
    "\n",
    "print(f\"Model initialized on device: {device}\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation: Temporal Split Strategy\n",
    "\n",
    "### Why Temporal Splitting Matters\n",
    "\n",
    "For time-series prediction problems, **random train/test splits are invalid** because they allow the model to \"see the future.\" Instead, we use a **temporal split**:\n",
    "\n",
    "```\n",
    "Timeline:  ──────────────────────────────────────────────────────────────▶\n",
    "           │  TRAINING WINDOW (hours 0-5)  │  PREDICTION TARGET (hours 6+)  │\n",
    "           │  \"What conditions existed?\"   │  \"Which nodes failed?\"          │\n",
    "```\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "1. **Features (X)**: Extract node conditions from the **early period** (hours 0-5)\n",
    "   - Load ratios, temperatures, status flags from WINTER_STORM_2021 telemetry\n",
    "   \n",
    "2. **Labels (y)**: Identify nodes that **eventually failed** in the late period (hours 6+)\n",
    "   - Binary: 1 = node failed at some point, 0 = node never failed\n",
    "\n",
    "This simulates the real-world task: \"Given current grid conditions, predict which nodes will fail in the coming hours.\"\n",
    "\n",
    "### Why WINTER_STORM_2021 for Training?\n",
    "\n",
    "- This scenario contains the most **cascade failure events** (stress conditions)\n",
    "- The model learns patterns that precede failures under extreme conditions\n",
    "- We then apply the trained model to other scenarios (BASE_CASE, HIGH_LOAD) during inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "prepare_training_data"
   },
   "outputs": [],
   "source": [
    "# prepare_training_data: Prepare training data from WINTER_STORM_2021 scenario\n",
    "train_scenario = 'WINTER_STORM_2021'\n",
    "train_telemetry = telemetry_df[telemetry_df['SCENARIO_NAME'] == train_scenario].copy()\n",
    "\n",
    "# Get unique timestamps\n",
    "timestamps = train_telemetry['TIMESTAMP'].unique()\n",
    "print(f\"Training scenario: {train_scenario}\")\n",
    "print(f\"Number of time steps: {len(timestamps)}\")\n",
    "\n",
    "# Create training samples (features from early time, labels from later time)\n",
    "training_data = []\n",
    "\n",
    "# Use data from hours 0-5 to predict failures at hours 6+\n",
    "early_timestamps = sorted(timestamps)[:24]  # First 6 hours (4 per hour)\n",
    "late_timestamps = sorted(timestamps)[24:]   # After hour 6\n",
    "\n",
    "# Features from early period\n",
    "early_telemetry = train_telemetry[train_telemetry['TIMESTAMP'].isin(early_timestamps)]\n",
    "X = create_node_features(nodes_df, early_telemetry)\n",
    "\n",
    "# Labels from late period (did node fail?)\n",
    "late_telemetry = train_telemetry[train_telemetry['TIMESTAMP'].isin(late_timestamps)]\n",
    "failure_nodes = late_telemetry[late_telemetry['STATUS'] == 'FAILED']['NODE_ID'].unique()\n",
    "\n",
    "y = torch.zeros(num_nodes, dtype=torch.float)\n",
    "for node_id in failure_nodes:\n",
    "    if node_id in node_to_idx:\n",
    "        y[node_to_idx[node_id]] = 1.0\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of failure nodes in labels: {int(y.sum())}\")\n",
    "print(f\"Failure nodes: {list(failure_nodes)[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "train_model"
   },
   "outputs": [],
   "source": [
    "# train_model: Train the GCN model\n",
    "# ============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "num_epochs = 100      # Number of full passes through the training data\n",
    "learning_rate = 0.01  # Step size for gradient descent (0.01 is typical for Adam)\n",
    "weight_decay = 5e-4   # L2 regularization coefficient (prevents large weights)\n",
    "\n",
    "# Move data to device (GPU if available, otherwise CPU)\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZER SETUP\n",
    "# ============================================================================\n",
    "# Adam optimizer combines:\n",
    "# - Momentum: accelerates convergence by accumulating gradient direction\n",
    "# - RMSprop: adapts learning rate per-parameter based on gradient magnitude\n",
    "# weight_decay adds L2 penalty: loss += weight_decay * sum(params^2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# ============================================================================\n",
    "# HANDLING CLASS IMBALANCE\n",
    "# ============================================================================\n",
    "# In cascade failures, most nodes survive (y=0) and few fail (y=1).\n",
    "# This creates class imbalance: model could achieve high accuracy by always\n",
    "# predicting \"no failure\" while missing all actual failures.\n",
    "#\n",
    "# Solution: Weight the positive class more heavily in the loss function.\n",
    "# pos_weight = (# negative samples) / (# positive samples)\n",
    "# Example: 95 surviving nodes, 5 failing → pos_weight = 19\n",
    "#          Each failure contributes 19x more to the loss gradient\n",
    "pos_weight = torch.tensor([(num_nodes - y.sum()) / max(y.sum(), 1)]).to(device)\n",
    "# NOTE: BCEWithLogitsLoss expects raw logits (pre-sigmoid), but our model\n",
    "# outputs probabilities (post-sigmoid). We use F.binary_cross_entropy instead.\n",
    "# The pos_weight computed here is informational; consider using it in a\n",
    "# weighted BCE if the model struggles with recall on failure cases.\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "model.train()  # Enable dropout and batch norm (if present)\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print(f\"Class imbalance ratio (pos_weight): {pos_weight.item():.2f}\")\n",
    "print(f\"Failure nodes: {int(y.sum().item())} / {num_nodes} ({100*y.sum().item()/num_nodes:.1f}%)\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "    \n",
    "    # Forward pass: compute predictions\n",
    "    out = model(X, edge_index)\n",
    "    \n",
    "    # Compute loss: Binary Cross-Entropy between predictions and labels\n",
    "    # BCE = -[y*log(p) + (1-y)*log(1-p)] averaged over all nodes\n",
    "    loss = F.binary_cross_entropy(out, y)\n",
    "    \n",
    "    # Backward pass: compute gradients via backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights: params -= learning_rate * gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        # Calculate accuracy (what fraction of predictions are correct)\n",
    "        # Note: accuracy can be misleading with class imbalance\n",
    "        pred = (out > 0.5).float()\n",
    "        correct = (pred == y).sum().item()\n",
    "        acc = correct / num_nodes\n",
    "        accuracies.append(acc)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics: Training Convergence\n",
    "\n",
    "Before running inference, we should verify the model trained properly by examining:\n",
    "1. **Loss curve**: Should decrease and plateau (not oscillate or diverge)\n",
    "2. **Confusion matrix**: Reveals if the model is biased toward predicting one class\n",
    "3. **Prediction distribution**: Probabilities should spread across [0, 1], not cluster at extremes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_training: Plot training loss curve and model diagnostics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Loss Curve ---\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, color='#2E86AB', linewidth=2, label='Training Loss')\n",
    "ax1.axhline(y=losses[-1], color='#A23B72', linestyle='--', alpha=0.7, \n",
    "            label=f'Final Loss: {losses[-1]:.4f}')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "ax1.set_title('Training Convergence', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation about convergence\n",
    "if losses[-1] < losses[0] * 0.5:\n",
    "    convergence_status = \"Good convergence\"\n",
    "    status_color = 'green'\n",
    "else:\n",
    "    convergence_status = \"May need more epochs\"\n",
    "    status_color = 'orange'\n",
    "ax1.text(0.95, 0.95, convergence_status, transform=ax1.transAxes, \n",
    "         fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "         bbox=dict(boxstyle='round', facecolor=status_color, alpha=0.3))\n",
    "\n",
    "# --- Prediction Distribution on Training Data ---\n",
    "ax2 = axes[1]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X, edge_index).cpu().numpy()\n",
    "\n",
    "# Separate predictions by actual label\n",
    "preds_positive = train_preds[y.cpu().numpy() == 1]\n",
    "preds_negative = train_preds[y.cpu().numpy() == 0]\n",
    "\n",
    "ax2.hist(preds_negative, bins=20, alpha=0.7, label=f'Actual Survived (n={len(preds_negative)})', \n",
    "         color='#28A745', edgecolor='white')\n",
    "ax2.hist(preds_positive, bins=20, alpha=0.7, label=f'Actual Failed (n={len(preds_positive)})', \n",
    "         color='#DC3545', edgecolor='white')\n",
    "ax2.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "ax2.set_xlabel('Predicted Failure Probability', fontsize=12)\n",
    "ax2.set_ylabel('Number of Nodes', fontsize=12)\n",
    "ax2.set_title('Prediction Distribution by True Label', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation guidance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Loss Curve:\")\n",
    "print(\"  • Decreasing loss indicates the model is learning\")\n",
    "print(\"  • Plateauing loss suggests convergence (good)\")\n",
    "print(\"  • Oscillating/increasing loss suggests learning rate issues\")\n",
    "print(\"\")\n",
    "print(\"Prediction Distribution:\")\n",
    "print(\"  • Ideal: Failed nodes (red) cluster near 1.0, survived (green) near 0.0\")\n",
    "print(\"  • Good separation = model discriminates well\")\n",
    "print(\"  • Overlapping distributions = harder classification task\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix: Evaluate model performance with confusion matrix and metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions at 0.5 threshold\n",
    "y_true = y.cpu().numpy()\n",
    "y_pred = (train_preds > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Confusion Matrix Heatmap ---\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "ax1.set_title('Confusion Matrix (Training Set)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax1.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=16, fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('Actual Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['Survived (0)', 'Failed (1)'])\n",
    "ax1.set_yticklabels(['Survived (0)', 'Failed (1)'])\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=ax1, shrink=0.8)\n",
    "\n",
    "# --- Metrics Bar Chart ---\n",
    "ax2 = axes[1]\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "bars = ax2.bar(metrics, values, color=colors, edgecolor='white', linewidth=2)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Classification Metrics', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed interpretation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX INTERPRETATION:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  True Negatives  (TN): {tn:4d} — Correctly predicted 'survived'\")\n",
    "print(f\"  False Positives (FP): {fp:4d} — Incorrectly predicted 'failed' (false alarm)\")\n",
    "print(f\"  False Negatives (FN): {fn:4d} — Missed actual failures (DANGEROUS)\")\n",
    "print(f\"  True Positives  (TP): {tp:4d} — Correctly predicted 'failed'\")\n",
    "print(\"\")\n",
    "print(\"KEY METRICS:\")\n",
    "print(f\"  • Precision = TP/(TP+FP) = {precision:.3f}\")\n",
    "print(f\"    'Of nodes we predicted would fail, what fraction actually did?'\")\n",
    "print(f\"  • Recall = TP/(TP+FN) = {recall:.3f}\")\n",
    "print(f\"    'Of nodes that actually failed, what fraction did we catch?'\")\n",
    "print(f\"  • F1 Score = 2*P*R/(P+R) = {f1:.3f}\")\n",
    "print(f\"    'Harmonic mean of precision and recall'\")\n",
    "print(\"\")\n",
    "print(\"FOR SAFETY-CRITICAL APPLICATIONS:\")\n",
    "print(\"  High RECALL is crucial — we must catch failures even at cost of false alarms\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference and Patient Zero Identification\n",
    "\n",
    "### What is \"Patient Zero\"?\n",
    "\n",
    "In epidemiology, Patient Zero is the first case that starts an outbreak. In power grid cascades, **Patient Zero is the first node to fail** — the domino that triggers the chain reaction.\n",
    "\n",
    "Identifying Patient Zero is critical for:\n",
    "- **Root cause analysis**: Understanding what initiated the cascade\n",
    "- **Prevention**: Reinforcing vulnerable nodes before they trigger failures\n",
    "- **Simulation**: Running \"what-if\" scenarios with different starting points\n",
    "\n",
    "### Our Heuristic for Finding Patient Zero\n",
    "\n",
    "Since we have the GCN's failure probability predictions, we use this approach:\n",
    "\n",
    "1. **Filter**: Consider only nodes that actually failed in the scenario\n",
    "2. **Rank**: Sort these by the model's predicted failure probability (descending)\n",
    "3. **Select**: The node with the highest probability is likely the origin\n",
    "\n",
    "**Intuition**: The model learned to assign high probabilities to nodes under stress. The failed node with the highest predicted probability was likely showing warning signs earliest → most likely to be the cascade origin.\n",
    "\n",
    "### Cascade Ordering via BFS\n",
    "\n",
    "Once we identify Patient Zero, we use **Breadth-First Search (BFS)** from that node to establish cascade order:\n",
    "\n",
    "```\n",
    "         Patient Zero\n",
    "             │\n",
    "    ┌────────┼────────┐\n",
    "    ▼        ▼        ▼\n",
    "  Node A   Node B   Node C   ← Cascade Order 2 (1-hop neighbors)\n",
    "    │        │\n",
    "    ▼        ▼\n",
    "  Node D   Node E            ← Cascade Order 3 (2-hop neighbors)\n",
    "```\n",
    "\n",
    "**Cascade Depth** = shortest path distance from Patient Zero. This indicates how quickly the failure propagated to reach each node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "batch_inference_all_scenarios"
   },
   "outputs": [],
   "source": [
    "# batch_inference_all_scenarios: Run inference for ALL scenarios\n",
    "model.eval()\n",
    "all_results = []\n",
    "\n",
    "scenarios = telemetry_df['SCENARIO_NAME'].unique()\n",
    "simulation_id = str(uuid.uuid4())[:8]\n",
    "run_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"Running batch inference for {len(scenarios)} scenarios...\")\n",
    "print(f\"Simulation ID: {simulation_id}\")\n",
    "\n",
    "# Build NetworkX graph for cascade analysis\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(node_ids)\n",
    "for _, row in edges_df.iterrows():\n",
    "    G.add_edge(row['SRC_NODE'], row['DST_NODE'])\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\nProcessing scenario: {scenario}\")\n",
    "    \n",
    "    # Get telemetry for this scenario\n",
    "    scenario_telemetry = telemetry_df[telemetry_df['SCENARIO_NAME'] == scenario]\n",
    "    \n",
    "    # Create features\n",
    "    X_scenario = create_node_features(nodes_df, scenario_telemetry).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_scenario, edge_index).cpu().numpy()\n",
    "    \n",
    "    # Identify actual failures from telemetry\n",
    "    failure_statuses = scenario_telemetry.groupby('NODE_ID')['STATUS'].apply(\n",
    "        lambda x: 'FAILED' in x.values\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Find Patient Zero (highest probability node that actually failed)\n",
    "    failure_probs = []\n",
    "    for idx, node_id in idx_to_node.items():\n",
    "        if failure_statuses.get(node_id, False):\n",
    "            failure_probs.append((node_id, predictions[idx]))\n",
    "    \n",
    "    failure_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "    patient_zero_id = failure_probs[0][0] if failure_probs else None\n",
    "    \n",
    "    # Calculate cascade order based on BFS from Patient Zero\n",
    "    cascade_order = {}\n",
    "    if patient_zero_id:\n",
    "        order = 1\n",
    "        for node in nx.bfs_tree(G, patient_zero_id):\n",
    "            if failure_statuses.get(node, False):\n",
    "                cascade_order[node] = order\n",
    "                order += 1\n",
    "    \n",
    "    # Generate results for each node\n",
    "    for idx, node_id in idx_to_node.items():\n",
    "        node_info = nodes_df[nodes_df['NODE_ID'] == node_id].iloc[0]\n",
    "        is_failed = failure_statuses.get(node_id, False)\n",
    "        is_patient_zero = (node_id == patient_zero_id)\n",
    "        \n",
    "        # Calculate impact metrics\n",
    "        if is_failed:\n",
    "            np.random.seed(hash(node_id) % 2**32)  # Deterministic per node\n",
    "            load_shed = node_info['CAPACITY_MW'] * np.random.uniform(0.3, 0.8)\n",
    "            customers = int(node_info['CAPACITY_MW'] * np.random.uniform(500, 1500))\n",
    "            repair_cost = node_info['CAPACITY_MW'] * np.random.uniform(5000, 15000)\n",
    "        else:\n",
    "            load_shed = 0\n",
    "            customers = 0\n",
    "            repair_cost = 0\n",
    "        \n",
    "        # Calculate cascade depth\n",
    "        c_order = cascade_order.get(node_id)\n",
    "        c_depth = None\n",
    "        if c_order is not None and patient_zero_id:\n",
    "            try:\n",
    "                c_depth = nx.shortest_path_length(G, patient_zero_id, node_id)\n",
    "            except:\n",
    "                c_depth = None\n",
    "        \n",
    "        # Generate AI explanation for high-risk nodes\n",
    "        ai_explanation = None\n",
    "        if predictions[idx] > 0.7 or is_patient_zero:\n",
    "            if is_patient_zero:\n",
    "                ai_explanation = f\"Identified as cascade origin. High criticality score ({node_info['CRITICALITY_SCORE']:.2f}) combined with network topology position makes this node a critical failure point.\"\n",
    "            else:\n",
    "                ai_explanation = f\"High failure risk due to proximity to cascade origin and load stress. Recommend preemptive load balancing.\"\n",
    "        \n",
    "        result = {\n",
    "            'SIMULATION_ID': f\"{simulation_id}_{scenario[:3]}\",\n",
    "            'SCENARIO_NAME': scenario,\n",
    "            'NODE_ID': node_id,\n",
    "            'RUN_TIMESTAMP': run_timestamp,\n",
    "            'FAILURE_PROBABILITY': float(predictions[idx]),\n",
    "            'IS_PATIENT_ZERO': is_patient_zero,\n",
    "            'CASCADE_ORDER': c_order,\n",
    "            'CASCADE_DEPTH': c_depth,\n",
    "            'LOAD_SHED_MW': round(load_shed, 2),\n",
    "            'CUSTOMERS_IMPACTED': customers,\n",
    "            'REPAIR_COST': round(repair_cost, 2),\n",
    "            'RISK_SCORE': round(predictions[idx] * node_info['CRITICALITY_SCORE'], 4),\n",
    "            'AI_EXPLANATION': ai_explanation\n",
    "        }\n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Summary for this scenario\n",
    "    scenario_results = [r for r in all_results if r['SCENARIO_NAME'] == scenario]\n",
    "    failed_count = sum(1 for r in scenario_results if r['CASCADE_ORDER'] is not None)\n",
    "    patient_zero = next((r for r in scenario_results if r['IS_PATIENT_ZERO']), None)\n",
    "    \n",
    "    print(f\"  - Nodes analyzed: {len(scenario_results)}\")\n",
    "    print(f\"  - Nodes in cascade: {failed_count}\")\n",
    "    if patient_zero:\n",
    "        print(f\"  - Patient Zero: {patient_zero['NODE_ID']} (prob: {patient_zero['FAILURE_PROBABILITY']:.4f})\")\n",
    "\n",
    "print(f\"\\nBatch inference complete!\")\n",
    "print(f\"Total results: {len(all_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_inference_results: Analyze and visualize batch inference results\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df_viz = pd.DataFrame(all_results)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- 1. Risk Score Distribution by Scenario ---\n",
    "ax1 = axes[0, 0]\n",
    "scenarios_colors = {'BASE_CASE': '#3498db', 'HIGH_LOAD': '#f39c12', 'WINTER_STORM_2021': '#e74c3c'}\n",
    "for scenario in results_df_viz['SCENARIO_NAME'].unique():\n",
    "    scenario_data = results_df_viz[results_df_viz['SCENARIO_NAME'] == scenario]['RISK_SCORE']\n",
    "    ax1.hist(scenario_data, bins=20, alpha=0.6, label=scenario, \n",
    "             color=scenarios_colors.get(scenario, 'gray'), edgecolor='white')\n",
    "ax1.set_xlabel('Risk Score (Probability × Criticality)', fontsize=11)\n",
    "ax1.set_ylabel('Number of Nodes', fontsize=11)\n",
    "ax1.set_title('Risk Score Distribution by Scenario', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- 2. Failure Probability Comparison ---\n",
    "ax2 = axes[0, 1]\n",
    "# Box plot of failure probability by scenario\n",
    "scenario_probs = [results_df_viz[results_df_viz['SCENARIO_NAME'] == s]['FAILURE_PROBABILITY'].values \n",
    "                  for s in results_df_viz['SCENARIO_NAME'].unique()]\n",
    "bp = ax2.boxplot(scenario_probs, labels=results_df_viz['SCENARIO_NAME'].unique(), patch_artist=True)\n",
    "colors = ['#3498db', '#f39c12', '#e74c3c']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax2.set_xlabel('Scenario', fontsize=11)\n",
    "ax2.set_ylabel('Failure Probability', fontsize=11)\n",
    "ax2.set_title('Failure Probability Distribution by Scenario', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# --- 3. Cascade Depth Analysis (for scenarios with failures) ---\n",
    "ax3 = axes[1, 0]\n",
    "winter_storm = results_df_viz[\n",
    "    (results_df_viz['SCENARIO_NAME'] == 'WINTER_STORM_2021') & \n",
    "    (results_df_viz['CASCADE_DEPTH'].notna())\n",
    "]\n",
    "if len(winter_storm) > 0:\n",
    "    depth_counts = winter_storm['CASCADE_DEPTH'].value_counts().sort_index()\n",
    "    ax3.bar(depth_counts.index, depth_counts.values, color='#e74c3c', edgecolor='white', alpha=0.8)\n",
    "    ax3.set_xlabel('Cascade Depth (hops from Patient Zero)', fontsize=11)\n",
    "    ax3.set_ylabel('Number of Failed Nodes', fontsize=11)\n",
    "    ax3.set_title('Cascade Propagation Depth (WINTER_STORM_2021)', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No cascade data available', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Cascade Propagation Depth', fontsize=13, fontweight='bold')\n",
    "\n",
    "# --- 4. Top Risk Nodes Table ---\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "top_risk = results_df_viz.nlargest(10, 'RISK_SCORE')[['NODE_ID', 'SCENARIO_NAME', 'FAILURE_PROBABILITY', 'RISK_SCORE', 'IS_PATIENT_ZERO']]\n",
    "top_risk['FAILURE_PROBABILITY'] = top_risk['FAILURE_PROBABILITY'].round(4)\n",
    "top_risk['RISK_SCORE'] = top_risk['RISK_SCORE'].round(4)\n",
    "top_risk['IS_PATIENT_ZERO'] = top_risk['IS_PATIENT_ZERO'].apply(lambda x: '★' if x else '')\n",
    "\n",
    "# Create table\n",
    "table = ax4.table(\n",
    "    cellText=top_risk.values,\n",
    "    colLabels=['Node ID', 'Scenario', 'Failure Prob', 'Risk Score', 'P0'],\n",
    "    loc='center',\n",
    "    cellLoc='center',\n",
    "    colWidths=[0.25, 0.3, 0.2, 0.15, 0.1]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.5)\n",
    "ax4.set_title('Top 10 Highest Risk Nodes', fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE RESULTS SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "for scenario in results_df_viz['SCENARIO_NAME'].unique():\n",
    "    s_data = results_df_viz[results_df_viz['SCENARIO_NAME'] == scenario]\n",
    "    high_risk = s_data[s_data['FAILURE_PROBABILITY'] > 0.7]\n",
    "    patient_zero = s_data[s_data['IS_PATIENT_ZERO'] == True]\n",
    "    \n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  • Total nodes: {len(s_data)}\")\n",
    "    print(f\"  • High-risk nodes (prob > 0.7): {len(high_risk)}\")\n",
    "    print(f\"  • Mean failure probability: {s_data['FAILURE_PROBABILITY'].mean():.4f}\")\n",
    "    print(f\"  • Max risk score: {s_data['RISK_SCORE'].max():.4f}\")\n",
    "    if len(patient_zero) > 0:\n",
    "        print(f\"  • Patient Zero: {patient_zero.iloc[0]['NODE_ID']}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "write_simulation_results"
   },
   "outputs": [],
   "source": [
    "# write_simulation_results: Write results to Snowflake\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Handle None values for Snowflake\n",
    "results_df['CASCADE_ORDER'] = results_df['CASCADE_ORDER'].apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "results_df['CASCADE_DEPTH'] = results_df['CASCADE_DEPTH'].apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "print(f\"Writing {len(results_df)} results to SIMULATION_RESULTS table...\")\n",
    "\n",
    "# Clear existing results and insert new ones\n",
    "session.sql(\"TRUNCATE TABLE IF EXISTS SIMULATION_RESULTS\").collect()\n",
    "\n",
    "# Convert to Snowpark DataFrame and write\n",
    "snowpark_df = session.create_dataframe(results_df)\n",
    "snowpark_df.write.mode('append').save_as_table('SIMULATION_RESULTS')\n",
    "\n",
    "# Verify write\n",
    "count = session.sql(\"SELECT COUNT(*) FROM SIMULATION_RESULTS\").collect()[0][0]\n",
    "print(f\"Successfully wrote {count} rows to SIMULATION_RESULTS\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample results:\")\n",
    "sample = session.sql(\"\"\"\n",
    "    SELECT SCENARIO_NAME, NODE_ID, FAILURE_PROBABILITY, IS_PATIENT_ZERO, CASCADE_ORDER\n",
    "    FROM SIMULATION_RESULTS\n",
    "    WHERE FAILURE_PROBABILITY > 0.5\n",
    "    ORDER BY SCENARIO_NAME, FAILURE_PROBABILITY DESC\n",
    "    LIMIT 10\n",
    "\"\"\").to_pandas()\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "save_model_artifacts"
   },
   "outputs": [],
   "source": [
    "# save_model_artifacts: Save model metadata to Snowflake\n",
    "import json\n",
    "\n",
    "# Training metrics\n",
    "metrics = {\n",
    "    'final_loss': round(losses[-1], 6),\n",
    "    'num_epochs': num_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'hidden_dim': 64,\n",
    "    'num_nodes': num_nodes,\n",
    "    'num_edges': edge_index.shape[1] // 2,\n",
    "    'random_seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "artifact_id = f\"gcn_cascade_{simulation_id}\"\n",
    "model_version = '1.0.0'\n",
    "metrics_json = json.dumps(metrics)\n",
    "\n",
    "# Insert model artifact record using SELECT with PARSE_JSON\n",
    "session.sql(f\"\"\"\n",
    "    INSERT INTO MODEL_ARTIFACTS (ARTIFACT_ID, MODEL_NAME, VERSION, TRAINING_SCENARIOS, METRICS, STATUS)\n",
    "    SELECT \n",
    "        '{artifact_id}',\n",
    "        'CascadeGCN',\n",
    "        '{model_version}',\n",
    "        'WINTER_STORM_2021',\n",
    "        PARSE_JSON('{metrics_json}'),\n",
    "        'ACTIVE'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Model artifact saved: {artifact_id}\")\n",
    "print(f\"Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "display_training_summary"
   },
   "outputs": [],
   "source": [
    "# display_training_summary: Final summary of training and inference\n",
    "print(\"=\"*60)\n",
    "print(\"GRIDGUARD - TRAINING & INFERENCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  - Architecture: GCN (3 layers)\")\n",
    "print(f\"  - Hidden dimension: 64\")\n",
    "print(f\"  - Input features: 10\")\n",
    "print(f\"  - Training epochs: {num_epochs}\")\n",
    "print(f\"  - Final loss: {losses[-1]:.6f}\")\n",
    "print(\"\")\n",
    "print(\"Data Summary:\")\n",
    "print(f\"  - Grid nodes: {num_nodes}\")\n",
    "print(f\"  - Grid edges: {edge_index.shape[1] // 2}\")\n",
    "print(f\"  - Scenarios processed: {len(scenarios)}\")\n",
    "print(\"\")\n",
    "print(\"Results by Scenario:\")\n",
    "\n",
    "summary = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        SCENARIO_NAME,\n",
    "        COUNT(*) AS NODES,\n",
    "        SUM(CASE WHEN CASCADE_ORDER IS NOT NULL THEN 1 ELSE 0 END) AS CASCADE_NODES,\n",
    "        ROUND(SUM(LOAD_SHED_MW), 0) AS TOTAL_LOAD_SHED_MW,\n",
    "        SUM(CUSTOMERS_IMPACTED) AS TOTAL_CUSTOMERS,\n",
    "        ROUND(SUM(REPAIR_COST), 0) AS TOTAL_REPAIR_COST\n",
    "    FROM SIMULATION_RESULTS\n",
    "    GROUP BY SCENARIO_NAME\n",
    "    ORDER BY SCENARIO_NAME\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Patient Zero Identification (WINTER_STORM_2021):\")\n",
    "patient_zero = session.sql(\"\"\"\n",
    "    SELECT NODE_ID, FAILURE_PROBABILITY, AI_EXPLANATION\n",
    "    FROM SIMULATION_RESULTS\n",
    "    WHERE SCENARIO_NAME = 'WINTER_STORM_2021' AND IS_PATIENT_ZERO = TRUE\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "if len(patient_zero) > 0:\n",
    "    pz = patient_zero.iloc[0]\n",
    "    print(f\"  - Node ID: {pz['NODE_ID']}\")\n",
    "    print(f\"  - Failure Probability: {pz['FAILURE_PROBABILITY']:.4f}\")\n",
    "    print(f\"  - Explanation: {pz['AI_EXPLANATION']}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"Training and inference complete!\")\n",
    "print(\"Results available in SIMULATION_RESULTS table.\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
